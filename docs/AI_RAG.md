# ü§ñ IA & RAG ‚Äî Meu Norte

## Vis√£o Geral

O assistente financeiro do Meu Norte utiliza **RAG (Retrieval-Augmented Generation)** para responder perguntas sobre os dados reais do usu√°rio. Todo o processamento √© local ‚Äî sem APIs externas.

```
Pergunta do usu√°rio
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Gera embedding  ‚îÇ  ‚Üê nomic-embed-text (Ollama)
‚îÇ da pergunta     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Busca similares ‚îÇ  ‚Üê pgvector (cosine similarity)
‚îÇ no banco        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Constr√≥i prompt ‚îÇ  ‚Üê prompt_builder.py
‚îÇ com contexto    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LLM gera        ‚îÇ  ‚Üê llama3.2 (Ollama)
‚îÇ resposta        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
  Stream via WebSocket ‚Üí Frontend
```

---

## Modelos Utilizados

| Modelo | Uso | Dimens√µes |
|--------|-----|-----------|
| `llama3.2` | Gera√ß√£o de respostas em linguagem natural | ‚Äî |
| `nomic-embed-text` | Gera√ß√£o de embeddings para indexa√ß√£o/busca | 768 |

---

## Pipeline de Indexa√ß√£o

Quando um lan√ßamento √© criado ou editado, uma **task Celery** √© disparada:

```python
# services/rag/pipeline.py
async def indexar_lancamento(lancamento_id: int):
    # 1. Buscar lan√ßamento no banco
    lancamento = await get_lancamento(lancamento_id)
    
    # 2. Montar texto descritivo
    texto = f"{lancamento.tipo}: {lancamento.descricao} - R${lancamento.valor:.2f} em {lancamento.data_vencimento}"
    
    # 3. Gerar embedding via Ollama
    embedding = await ollama_client.embed(texto, model="nomic-embed-text")
    
    # 4. Salvar no pgvector
    await salvar_embedding(lancamento_id, embedding, texto)
```

---

## Retrieval (Busca)

```python
# Busca os 5 lan√ßamentos mais semanticamente similares √† query
async def buscar_contexto(query: str, user_id: int, top_k: int = 5):
    query_embedding = await ollama_client.embed(query)
    
    stmt = select(LancamentoEmbedding, Lancamento) \
        .join(Lancamento) \
        .where(Lancamento.user_id == user_id) \
        .order_by(LancamentoEmbedding.embedding.cosine_distance(query_embedding)) \
        .limit(top_k)
    
    return await db.execute(stmt)
```

---

## Constru√ß√£o do Prompt

```python
# services/rag/prompt_builder.py
def construir_prompt(query: str, contexto: list, usuario: str) -> str:
    contexto_str = "\n".join([
        f"- {item.tipo.title()}: {item.descricao} "
        f"(R$ {item.valor:.2f}, {item.status}, {item.data_vencimento})"
        for item in contexto
    ])
    
    return f"""Voc√™ √© um assistente financeiro pessoal do usu√°rio {usuario}.
    
Com base nos lan√ßamentos financeiros abaixo, responda √† pergunta do usu√°rio de forma clara e objetiva em portugu√™s:

CONTEXTO FINANCEIRO:
{contexto_str}

PERGUNTA: {query}

RESPOSTA:"""
```

---

## WebSocket ‚Äî Chat em Tempo Real

O frontend conecta via WebSocket para receber a resposta em streaming:

```typescript
// hooks/useChatWebSocket.ts
const ws = new WebSocket(`ws://localhost:8000/api/v1/chat/ws?token=${token}`);

ws.onmessage = (event) => {
    const data = JSON.parse(event.data);
    if (data.type === 'token') {
        // Adiciona token √† mensagem atual (efeito "digitando")
        setCurrentMessage(prev => prev + data.content);
    }
};

ws.send(JSON.stringify({ message: userMessage }));
```

---

## Como Configurar o Ollama

```bash
# 1. Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Baixar os modelos necess√°rios
ollama pull llama3.2
ollama pull nomic-embed-text

# 3. Verificar
ollama list
```

No `docker-compose.yml`, o backend conecta ao Ollama do host via:
```yaml
OLLAMA_URL: http://host.docker.internal:11434
# No Linux: http://172.17.0.1:11434
```

---

## Performance

| Opera√ß√£o | Tempo m√©dio |
|----------|-------------|
| Gera√ß√£o de embedding (nomic) | ~0.3s |
| Busca vetorial (pgvector) | < 50ms |
| Gera√ß√£o de resposta (llama3.2) | 2-8s (dependendo do hardware) |
| Indexa√ß√£o (background) | ~0.5s total |

> üí° Com GPU dispon√≠vel, o tempo de gera√ß√£o de resposta cai para < 1s
